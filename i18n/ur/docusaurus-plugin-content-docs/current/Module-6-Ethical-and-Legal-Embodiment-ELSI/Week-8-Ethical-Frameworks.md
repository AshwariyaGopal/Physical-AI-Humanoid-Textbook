---
id: week-8-ethical-frameworks
title: اخلاقی فریم ورکس
sidebar_label: ہفتہ 8 اخلاقی فریم ورکس
---

## 1. تعارف: ہیومنائیڈ کی پہیلی

ہیومنائیڈ روبوٹکس اور مصنوعی ذہانت کی تیز رفتار ترقی اخلاقی اور سماجی چیلنجز کا ایک منفرد سیٹ پیش کرتی ہے۔ جیسے جیسے روبوٹ زیادہ نفیس، خود مختار اور روزمرہ کی زندگی میں مربوط ہوتے جا رہے ہیں، ان کے انسانی تعامل، اخلاقی ذمہ داری اور سماجی اصولوں پر پڑنے والے اثرات کے بارے میں سوالات اٹھتے ہیں۔ یہ باب مجسم AI کے اخلاقی، قانونی، اور سماجی مضمرات (ELSI) کا گہرائی سے مطالعہ کرتا ہے، یہ دریافت کرتا ہے کہ ہم ہیومنائیڈز کو ذمہ داری کے ساتھ کیسے ڈیزائن، تیار اور تعینات کر سکتے ہیں۔

## 2. نفسیاتی اثر: HRI میں اینتھروپومورفزم اور انکنی ویلی

انسان فطری طور پر غیر انسانی ہستیوں کو انسانی جیسی خصوصیات منسوب کرنے کی طرف مائل ہوتے ہیں۔ یہ رجحان، جسے **اینتھروپومورفزم** کہا جاتا ہے، روبوٹس کو ہم کیسے سمجھتے ہیں اور ان سے کیسے تعامل کرتے ہیں اس میں ایک اہم کردار ادا کرتا ہے۔ تاہم، ایک روبوٹ میں انسانی جیسی خصوصیات کی سطح پیچیدہ اور بعض اوقات غیر متوقع نفسیاتی اثرات مرتب کر سکتی ہے، جسے خاص طور پر **انکنی ویلی** سے واضح کیا گیا ہے۔

### 2.1. اینتھروپومورفزم

**اینتھروپومورفزم** انسانی خصوصیات، جذبات، یا ارادوں کو غیر انسانی ہستیوں سے منسوب کرنا ہے۔ ہیومن-روبوٹ انٹرایکشن (HRI) میں، ڈیزائنرز اکثر اینتھروپومورفزم کا فائدہ اٹھاتے ہیں تاکہ روبوٹس کو زیادہ بدیہی، قابل رسائی، اور دلکش بنایا جا سکے۔

*   **فوائد**: اعتماد میں اضافہ، بہتر مواصلات، بہتر صارف کا تجربہ، اور رفاقت کا احساس۔ مثال کے طور پر، اظہار کن چہروں یا اشاروں کی مواصلت والے روبوٹس انسانوں کے لیے سمجھنے اور تعاون کرنے میں آسان ہو سکتے ہیں۔
*   **خطرات**: ذہانت یا احساس کی زیادہ خصوصیت، جس کے نتیجے میں جھوٹی توقعات، جذباتی ہیرا پھیری، یا روبوٹ کے تئیں انسانی ذمہ داری کا کم احساس ہوتا ہے۔ صارفین جذباتی لگاؤ پیدا کر سکتے ہیں یا، اس کے برعکس، جب روبوٹ انسانی جیسی توقعات پر پورا نہیں اترتا تو دھوکہ دہی محسوس کر سکتے ہیں۔

### 2.2. انکنی ویلی

**انکنی ویلی** جمالیات میں ایک مفروضہ ہے جو یہ بتاتا ہے کہ جیسے جیسے ایک روبوٹ (یا دیگر غیر انسانی ہستی) کی ظاہری شکل زیادہ انسانی جیسی ہوتی جاتی ہے، اس کی کشش بڑھتی جاتی ہے، لیکن صرف ایک خاص حد تک۔ اس نقطہ سے آگے، اس کی انسان سے مشابہت پریشان کن، عجیب، یا "انکنی" ہو جاتی ہے۔ تاہم، اگر مشابہت بڑھتی رہتی ہے اور انسان سے تقریباً ناقابل امتیاز ہو جاتی ہے، تو کشش ایک بار پھر بڑھ جاتی ہے۔

*   **رجحان کی تفصیل**: "ویلی" انسانی تعلق بمقابلہ انسانی مشابہت کے گراف میں ڈپ ہے۔ یہ اکثر معمولی خامیوں کی وجہ سے ہوتا ہے جو روبوٹ کو "تقریباً انسانی" لیکن مکمل طور پر نہیں دکھاتا ہے، جس سے بے چینی، گھن، یا خوف کے احساسات پیدا ہوتے ہیں۔
*   **نفسیاتی بنیاد**: نظریات بتاتے ہیں کہ یہ اس سے پیدا ہو سکتا ہے:
    *   **درجہ بندی کی مشکل**: دماغ ہستی کو انسانی یا غیر انسانی کے طور پر درجہ بندی کرنے میں جدوجہد کرتا ہے۔
    *   **خطرہ کا پتہ لگانا**: انسانی اصولوں سے معمولی انحراف بیماری، موت، یا ایک فریب کن خطرے کا اشارہ دے سکتا ہے۔
    *   **توقعات کی خلاف ورزی**: روبوٹ انسانی لگتا ہے لیکن حرکت یا برتاؤ بالکل انسان جیسا نہیں کرتا۔
*   **ڈیزائن کے مضمرات**: انکنی ویلی کو نیویگیٹ کرنا مثبت HRI حاصل کرنے کے لیے اہم ہے، خاص طور پر ان روبوٹس کے لیے جو سماجی کرداروں کے لیے مقصود ہیں۔

### 2.3. انکنی ویلی کو نیویگیٹ کرنے کے لیے ڈیزائن کے اصول

ڈیزائنرز انکنی ویلی میں گرنے سے بچنے کے لیے حکمت عملیوں کا استعمال کر سکتے ہیں:
*   **اسٹائلائزیشن کو اپنائیں**: روبوٹس کو واضح روبوٹک خصوصیات کے ساتھ ڈیزائن کریں، غیر ضروری طور پر حقیقت پسندانہ انسانی جلد کی بناوٹ یا چہرے کی تفصیلات سے گریز کریں جب تک کہ کمال حاصل نہ کیا جا سکے۔
*   **فنکشن پر توجہ دیں**: روبوٹ کی شکل کو اس کے فنکشن کی پیروی کرنے دیں، انسانی جیسی خصوصیات کو مواصلات کے لیے ارادے کے ساتھ استعمال کیا جائے، نہ کہ محض نقل کے لیے۔
*   **برتاؤ کی مطابقت**: یقینی بنائیں کہ روبوٹ کی حرکات، تاثرات، اور تقریر اس کی ظاہری شکل سے مطابقت رکھتی ہے۔ عدم مطابقت انکنی اثر کو بڑھا سکتی ہے۔
*   **شفافیت**: روبوٹ کی مصنوعی نوعیت کو صارفین کے لیے واضح کریں، توقعات کا انتظام کریں۔

## 3. بنیادی روبوٹکس کے فریم ورکس

روبوٹکس کے لیے اخلاقیات کے تصورات نے نمایاں طور پر ارتقاء پایا ہے، قیاس آرائی پر مبنی سوچ کے تجربات سے ترقی کے لیے عملی رہنما اصولوں کی طرف بڑھتے ہوئے ہیں۔

### 3.1. آسیموف کے روبوٹکس کے قوانین اور ان کی حدود

آئزک آسیموف کے روبوٹکس کے تین قوانین، جو پہلی بار 1940 کی دہائی میں شائع ہوئے، نے روبوٹ کے رویے کے لیے ایک ابتدائی اور بااثر فریم ورک فراہم کیا:
1.  ایک روبوٹ کسی انسان کو نقصان نہیں پہنچا سکتا، یا غیر فعالیت کے ذریعے، کسی انسان کو نقصان پہنچنے نہیں دے سکتا۔
2.  ایک روبوٹ کو انسانوں کی طرف سے دیے گئے احکامات کی تعمیل کرنی چاہیے سوائے اس کے کہ جب ایسے احکامات پہلے قانون سے متصادم ہوں۔
3.  ایک روبوٹ کو اپنے وجود کی حفاظت کرنی چاہیے جب تک کہ ایسی حفاظت پہلے یا دوسرے قانون سے متصادم نہ ہو۔

اگرچہ یہ قوانین بنیادی ہیں، لیکن یہ **قاعدہ پر مبنی نظام** ہیں اور پیچیدہ حقیقی دنیا کے منظرناموں میں ان کی نمایاں حدود ہیں:
*   **ابہام**: "نقصان" کیا ہے؟ متصادم احکامات کو کیسے حل کیا جاتا ہے؟
*   **ترجیح**: قوانین ایک سخت درجہ بندی کا مطلب دیتے ہیں، لیکن حقیقی دنیا کی اخلاقیات اکثر باریک ہوتی ہیں۔
*   **خود مختاری**: وہ یہ فرض کرتے ہیں کہ روبوٹ تابع ہیں، خود مختار ایجنٹس نہیں جن کی اپنی سیکھنے کی صلاحیتیں ہیں۔
*   **غیر ارادی نتائج**: قوانین کی سخت پابندی غیر متوقع اخلاقی مخمصوں کا باعث بن سکتی ہے (مثلاً، ایک روبوٹ کسی انسان کو پراپرٹی کو تباہ کرکے بچاتا ہے، یا ایسا فیصلہ کرتا ہے جو بالواسطہ طور پر دوسرے کو نقصان پہنچاتا ہے)۔

### 3.2. مشین اخلاقیات: اخلاقی استدلال کو انکوڈ کرنا

**مشین اخلاقیات** مصنوعی ایجنٹس کو اخلاقی طور پر برتاؤ کرنے کے لیے ڈیزائن کرنے سے متعلق تحقیق کا ایک شعبہ ہے۔ آسیموف کے جیسے سخت کوڈ شدہ اصولوں پر انحصار کرنے کے بجائے، اس کا مقصد مشینوں کو اخلاقی استدلال کی صلاحیت سے آراستہ کرنا ہے، جس سے وہ نئے حالات میں اخلاقی فیصلے کر سکیں۔

نقطہ نظر میں شامل ہیں:
*   **اوپر سے نیچے**: اخلاقی اصولوں کو براہ راست مشین میں پروگرام کرنا۔
*   **نیچے سے اوپر**: مشینوں کو ڈیٹا یا مشاہدے (مثلاً، reinforcement learning کے ذریعے) سے اخلاقی رویہ سیکھنے کی اجازت دینا۔
*   **ہائبرڈ نقطہ نظر**: واضح اصولوں کو سیکھے ہوئے رویے کے ساتھ جوڑنا۔

### 3.3. جدید تنقیدیں اور اصول (مثلاً، IEEE اصول)

عصری روبوٹکس کے فریم ورکس، جیسے کہ IEEE گلوبل انیشی ایٹو برائے AI اور خود مختار سسٹمز میں اخلاقی تصورات، سادہ اصولوں سے وسیع اصولوں کی طرف بڑھتے ہیں۔ یہ اکثر زور دیتے ہیں:
*   **انسانی حقوق**: یقینی بنانا کہ AI انسانی حقوق کا احترام کرے۔
*   **فلاح و بہبود**: انسانی فلاح و بہبود اور بہبود کو ترجیح دینا۔
*   **احتساب**: AI کے اعمال کے لیے واضح ذمہ داری قائم کرنا۔
*   **شفافیت**: AI کے فیصلہ سازی کے عمل کو قابل فہم بنانا۔
*   **پرائیویسی**: ذاتی ڈیٹا کی حفاظت کرنا۔
*   **انصاف**: تعصب کو روکنا اور کم کرنا۔

## 4. ڈیزائن میں تعصب اور مساوات

چونکہ ہیومنائیڈز انسانوں کے ذریعہ ڈیزائن کیے جاتے ہیں اور انسانی پیدا کردہ ڈیٹا پر تربیت یافتہ ہوتے ہیں، اس لیے وہ موجودہ سماجی تعصبات کو وراثت میں حاصل کرنے اور انہیں بڑھانے کا خطرہ رکھتے ہیں، جس کے نتیجے میں غیر مساوی نتائج برآمد ہوتے ہیں۔

### 4.1. تعصب کے ذرائع

تعصب ہیومنائیڈ ڈیزائن کے مختلف پہلوؤں میں ظاہر ہو سکتا ہے:
*   **آواز**: صنفی آوازیں (مثلاً، نسائی کوڈڈ اسسٹنٹس)، ثقافتی طور پر مخصوص لہجے، جس کے نتیجے میں Stereotypes یا غلط نمائندگی ہوتی ہے۔
*   **चाल**: روبوٹس کو متعصبانہ مفروضوں کی بنیاد پر "نسوانی" یا "مردانہ" چالوں سے آراستہ کرنا۔
*   **ظاہری شکل**: روبوٹس کو مخصوص نسلی، صنفی، یا ثقافتی جمالیات کے ساتھ ڈیزائن کرنا جو Stereotypes کو برقرار رکھتے ہیں، یا ایک "ڈیفالٹ" انسانی شکل بنانا جو دوسروں کو خارج کرتی ہے۔
*   **تربیتی ڈیٹا**: اگر پرسیپشن، قدرتی زبان کی پروسیسنگ، یا رویے کی تخلیق کے لیے تربیتی ڈیٹا سیٹ غیر نمائندہ ہوں یا ان میں تعصبات ہوں، تو روبوٹ ان تعصبات کو سیکھے گا اور ظاہر کرے گا۔

### 4.2. غیر منصفانہ ڈیزائن کا اثر

متعصبانہ ڈیزائن اس کا باعث بن سکتا ہے:
*   **اخراج**: روبوٹس جو بعض آبادیاتی گروہوں کے ساتھ مؤثر طریقے سے یا احترام کے ساتھ تعامل کرنے میں ناکام رہتے ہیں۔
*   **امتیاز**: نقصان دہ Stereotypes کو تقویت دینا یا پسماندہ گروہوں کے خلاف فعال طور پر امتیازی سلوک کرنا۔
*   **اعتماد کا خاتمہ**: صارفین کا روبوٹک سسٹمز کی انصاف پسندی اور غیر جانبداری پر سے اعتماد اٹھ جانا۔

### 4.3. تعصب کم کرنے کی حکمت عملی

*   **شامل ڈیزائن کے طریقہ کار**: ڈیزائن کے عمل میں متنوع صارف گروہوں کو شامل کرنا۔
*   **متنوع ڈیزائن ٹیمیں**: یقینی بنانا کہ تخلیق کار خود مختلف پس منظر سے آتے ہیں۔
*   **تربیتی ڈیٹا کا اہم جائزہ**: DRL یا دیگر سیکھنے کے الگورتھم کے لیے استعمال ہونے والے ڈیٹا سیٹس میں تعصبات کو فعال طور پر تلاش کرنا اور ان کی اصلاح کرنا۔
*   **شفافیت اور قابل تشریحیت**: یہ واضح کرنا کہ روبوٹ کسی خاص طریقے سے کیوں برتاؤ کرتا ہے تاکہ متعصبانہ فیصلوں کی نشاندہی اور ان کی اصلاح کی جا سکے۔
*   **کونٹیکسٹ-آگاہ تعیناتی**: اس سماجی سیاق و سباق کو سمجھنا جس میں روبوٹ کام کرتے ہیں اور ممکنہ تعصبات کی پیش گوئی کرنا۔

:::warning
**متعصبانہ تربیتی ڈیٹا سے خبردار رہیں**: ڈیپ لرننگ ماڈلز، بشمول روبوٹ پرسیپشن اور کنٹرول کے لیے استعمال ہونے والے، صرف اتنے ہی غیر متعصب ہوتے ہیں جتنا کہ وہ ڈیٹا جس پر انہیں تربیت دی جاتی ہے۔ متعصبانہ ڈیٹا سیٹ ایسے ماڈلز کا باعث بن سکتے ہیں جو Stereotypes کو برقرار رکھتے ہیں، غیر منصفانہ فیصلے کرتے ہیں، یا یہاں تک کہ بعض صارف گروہوں کو نقصان پہنچاتے ہیں۔ باقاعدہ آڈٹ اور متنوع ڈیٹا اکٹھا کرنے کے طریقے اہم ہیں۔
:::